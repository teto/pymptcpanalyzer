import logging
import os
import pandas as pd
import numpy as np
from mptcpanalyzer.tshark import TsharkConfig, Field, _convert_timestamp
from mptcpanalyzer.connection import MpTcpSubflow, MpTcpConnection, TcpConnection, \
        MpTcpMapping, TcpMapping, swap_role, TcpStreamId, MpTcpStreamId
import mptcpanalyzer as mp
from mptcpanalyzer import (RECEIVER_SUFFIX, SENDER_SUFFIX, _receiver, _sender,
HOST1_SUFFIX, HOST2_SUFFIX,
_first, _second,
suffix_fields, get_config, get_cache, ConnectionRoles)
from typing import List, Any, Tuple, Dict, Callable, Collection, Union
import math
import tempfile
import pprint
from enum import Enum, auto
import functools
from mptcpanalyzer.debug import debug_dataframe

log = logging.getLogger(__name__)

pp = pprint.PrettyPrinter(indent=4)


# TODO might need a converter when saving/loading
# TODO pandas.api.types.register_extension_dtype()
# the list here is important in fact
dtype_role = pd.api.types.CategoricalDtype(categories=list(ConnectionRoles), ordered=True)

TCP_DEBUG_FIELDS = ['packetid', "reltime", "abstime"]
MPTCP_DEBUG_FIELDS = TCP_DEBUG_FIELDS + ['mptcpdest']


def _convert_role(x):
    """
    Workaround https://github.com/pandas-dev/pandas/pull/20826
    """
    log.log(mp.TRACE, "converting [%r] into role" % x)
    return ConnectionRoles(x)

def ignore(f1, f2):
    return 0

def exact(f1, f2):
    return 10 if (math.isnan(f1) and math.isnan(f2)) or f1 == f2 else float('-inf')

def diff(f1, f2):
    return f2 - f1

def debug_convert(df):
    return df
    # return df.head(20)


def getrealpath(input_file):
    filename = os.path.expanduser(input_file)
    filename = os.path.realpath(filename)
    return filename


"""
when trying to map packets from a pcap to another, we give a score to each mapping
based on per-field rules.

invariant: True if not modified by the network
Of the form Field.shortname

Have a look at the graphic slide 28:
https://www-phare.lip6.fr/cloudnet12/Multipath-TCP-tutorial-cloudnet.pptx

"""
scoring_rules = {
    "packetid": ignore,
    # in-order packets are more common than out of order ones
    "abstime": diff,
    "default_time": ignore,
    "expected_token": exact,
    "sport": exact,
    "dport": exact,
    "rwnd": exact,
    "sendkey": exact,
    "rcvkey": exact,
    "rcvtoken": exact,
    "tcpflags": exact,
    "dss_dsn": exact,
    "dss_rawack": exact,
    "dss_ssn": exact,
    "tcpseq": exact,
    "tcplen": exact,
}

"""
On top of Tshark fields, we also describe fields generated by mptcpanalyzer
"""
per_pcap_artificial_fields = {
    "mptcpdest": Field("mptcpdest", dtype_role, "MPTCP destination", False, None),
    "tcpdest": Field("tcpdest", dtype_role, "TCP destination", False, None),

    # "mptcpdest": Field("mptcpdest", None, "MPTCP destination", False, _convert_role),
    # "tcpdest": Field("tcpdest", None, "TCP destination", False, _convert_role),

    # TODO use int? as type
    "hash": Field("hash", str, "Hash of fields", False, None),

    # TODO rename ?
    # TODO should be a CategoryDataType !
    # "merge": Field("_merge", None, "How many packets were merged", False, None)
}

# merged_per_pcap_artificial_fields = {
#     "_merge": Field("merge", str, None, False, None)

class PacketMappingMode(Enum):
    """
    How to map packets from one stream to another

    The SCORE based algorithm allows to work with traffic that went trhoug NATs
    etc but is buggy/less tested

    The hash based is more straightforward
    """
    HASH = auto()
    SCORE = auto()


def check_df(f, checks):
    '''
    decorator checking that dataframe fulfill some conditions
    first argument (dataframe) 
    '''
    # TODO
    @functools.wraps(f)
    def wrapped(self, *args):
        if self.data is not None:
            return f(self, *args)
        else:
            raise mp.MpTcpException("Please load a pcap with `load_pcap` first")
        return None
    return wrapped


def drop_syn(df: pd.DataFrame, mptcp: bool = True) -> pd.DataFrame:
    """
    for mptcp it's as easy as removing packets with MP_CAPABLE or MP_JOIN
    """

    # syns = df[df.tcpflags == mp.TcpFlags.SYN]

    syn_index = df.tcpflags.where(lambda x: x & mp.TcpFlags.SYN).dropna()
    return df.drop(syn_index.Index)


def load_merged_streams_into_pandas(
    pcap1: str,
    pcap2: str,
    streamid1: int,
    streamid2: int,
    mptcp: bool,
    tshark_config: TsharkConfig,
    clock_offset1: int = 0,
    clock_offset2: int = 0,
    mapping_mode: PacketMappingMode = PacketMappingMode.HASH,
    **extra
    ):
    """
    Arguments:
        protocol: mptcp or tcp
        mapping_mode: Only HASH works for now
        clock_offset: untested

    Returns
        a dataframe with columns... owd ?
    """
    log.debug("Asked to load merged tcp streams %d and %d from pcaps %s and %s"
        % (streamid1, streamid2, pcap1, pcap2)
    )

    cache = mp.get_cache()
    protocolStr = "mptcp" if mptcp else "tcp"

    cacheid = cache.cacheuid("merged", [ getrealpath(pcap1), getrealpath(pcap2),],
        protocolStr + "_" + str(streamid1) + "_" + str(streamid2) + ".csv")

    # if we can't load that file from cache
    try:
        merged_df = pd.DataFrame()
        res = pd.DataFrame()

        valid, cachename = cache.get(cacheid)
        log.info("Cache validity=%s and cachename=%s" % (valid, cachename))

        # TODO disable when clock_offset is set
        if not valid:
            df1 = load_into_pandas(pcap1, tshark_config, clock_offset=clock_offset1)
            df2 = load_into_pandas(pcap2, tshark_config, clock_offset=clock_offset2)

            main_connection = None  # type: Union[MpTcpConnection, TcpConnection]
            other_connection = None  # type: Union[MpTcpConnection, TcpConnection]
            if mptcp:
                main_connection = MpTcpConnection.build_from_dataframe(df1, MpTcpStreamId(streamid1))
                other_connection = MpTcpConnection.build_from_dataframe(df2, MpTcpStreamId(streamid2))

                # TODO generate
                # map_mptcp_connection()

                # for now we use known streams exclusively
                # might be interested to use merge_tcp_dataframes later
                merged_df = merge_mptcp_dataframes_known_streams(
                    (df1, main_connection),
                    (df2, other_connection)
                )

            else:
                main_connection = TcpConnection.build_from_dataframe(df1, TcpStreamId(streamid1))
                other_connection = TcpConnection.build_from_dataframe(df2, TcpStreamId(streamid2))

                # for now we use known streams exclusively
                # might be interested to use merge_tcp_dataframes later
                merged_df = merge_tcp_dataframes_known_streams(
                    (df1, main_connection),
                    (df2, other_connection)
                )

            assert cachename
            log.info("Saving into %s" % cachename)
            # trying to export lists correctly
            # print(merged_df.reinjected_in.dropna().head())
            # convert arrays back to strings
            # merged_df.apply(",".join()
            # or abstime ?

            merged_df.to_csv(
                cachename,
                # columns=columns,
                index=False,
                header=True,
                sep=tshark_config.delimiter,
            )

            # tcpdest had become an objected instead of a CategoricalDtype
            # see https://github.com/pandas-dev/pandas/issues/22361
            log.log(mp.TRACE, "saving with dtypes=", dict(merged_df.dtypes))

        else:
            log.info("Loading from cache %s" % cachename)
            # dtypes = {k: v for k, v in temp.items() if v is not None or k not in ["tcpflags"]}

            def _gen_dtypes(fields) -> Dict[str, Any]:
                dtypes = {}  # type: ignore
                for _name in [_first, _second]:

                    # TODO this could be simplified
                    for k, v in fields.items():
                        if v is not None or k not in ["tcpflags"]:
                            dtypes.setdefault(_name(k), v)

                    # add generated field dtypes
                    dtypes.update({_name(f.fullname): f.type for f in per_pcap_artificial_fields.values()})

                # these are overrides from the generated dtypes
                # dtypes.update({
                #     _first("packetid"): tshark_config.fields["packetid"].type,
                #     _second("packetid"): tshark_config.fields["packetid"].type,
                # })

                return dtypes

            def _gen_converters() -> Dict[str, Callable]:

                # converters = {}   # type: Dict[str, Any]
                fields = dict(tshark_config.fields)
                fields.update(per_pcap_artificial_fields)
                converters = {}
                # tcpflags is already in good format
                default_converters = {name: f.converter for name, f in fields.items()
                    if f.converter and name != "tcpflags"}
                # converters.update({ name: f.converter for name, f in per_pcap_artificial_fields.items() if f.converter})
                for name, converter in default_converters.items():
                    converters.update({_first(name): converter, _second(name): converter})

                return converters

            with open(cachename) as fd:
                merge_dtypes = _gen_dtypes(
                    {name: field.type for name, field in tshark_config.fields.items() if field.converter is None}
                )
                converters = _gen_converters()

                log.log(mp.TRACE, "Using dtypes %s" % pp.pformat(merge_dtypes))
                log.log(mp.TRACE, "Using converters %s" % (pp.pformat(converters)))
                merged_df = pd.read_csv(
                    fd,
                    skip_blank_lines=True,
                    comment='#',
                    # we don't need 'header' when metadata is with comment
                    sep=tshark_config.delimiter,
                    # memory_map=True, #Â could speed up processing
                    dtype=merge_dtypes,  # poping still generates
                    converters=converters,
                )
                # at this stage, destinatiosn are nan
                # debug_dataframe(merged_df, "Merged dataframe", )

                # workaround bug https://github.com/pandas-dev/pandas/issues/25448
                def _convert_to_enums():
                    # per_pcap_artificial_fields
                    for col in [ _first("tcpdest"), _first("mptcpdest"), _second("tcpdest"), _second("mptcpdest")]:
                        merged_df[col] = merged_df[col].apply(_convert_role, convert_dtype=False)

                # log.debug("Column names after loading from cache: %s", merged_df.columns)

        # we fix the clocks a posteriori so that the cache is still usable
        log.debug("Postprocessing clock if needed")
        merged_df[_first('abstime')] += clock_offset1
        merged_df[_second('abstime')] += clock_offset2

        log.debug("Converting dataframes to be sender/receiver based...")
        # in both cases
        # TODO here we should attribute the definite mptcprole
        # compute owd
        if mptcp:
            print("Should be merging OWDs")
            log.error("We should correct the clocks if the argument is passed !")
            # raise mp.MpTcpException("Implement mptcp merge")

            res = convert_to_sender_receiver(merged_df)
        else:
            # tcp
            res = convert_to_sender_receiver(merged_df)

        # log.debug("Sorting by sender abstime")
        # merged_df.sort_values(by=_sender("abstime"), ascending=True, inplace=True)

        # debug_dataframe(res, "checking merge", usecols=["merge_status"])
        # print("%d nan values" % len(res[res.merge_status == np.nan]))

        # log.debug("Column names: %s", res.columns)
        # log.debug("Dtypes after load:%s\n" % dict(res.dtypes))

        log.debug("Computing owds")
        # TODO we don't necessarely need to generate the OWDs here, might be put out
        res['owd'] = res[_receiver('abstime')] - res[_sender('abstime')]

        debug_dataframe(res, "owd", usecols=["owd", _sender('abstime'), _receiver('abstime')] )
        # with pd.option_context('float_format', '{:f}'.format):
        #     print(
        #         res[_sender(["ipsrc", "ipdst", "abstime"]) + _receiver(["abstime", "packetid"]) + TCP_DEBUG_FIELDS + ["owd"] ]
        #     )

    except Exception:
        log.exception("exception happened while merging")

    # pd.set_option('display.max_rows', 200)
    # pd.set_option('display.max_colwidth', -1)
    # print("dtypes=", dict(dtypes))
    log.log(mp.TRACE, "Dtypes after load:%s\n" % pp.pformat(res.dtypes))
    log.info("Finished loading. merged dataframe size: %d" % len(res))

    return res


def load_into_pandas(
    input_file: str,
    config: TsharkConfig,
    **extra
) -> pd.DataFrame:
    """
    load mptcp  data into pandas

    Args:
        input_file: pcap filename
        config: Hard, keep changing
        load_cb: callback to use if cache not available
        extra: extra arguments to forward to load_cb
    """
    log.debug("Asked to load simple pcap %s" % input_file)

    filename = getrealpath(input_file)
    cache = mp.get_cache()

    tshark_dtypes = {fullname: field.type for fullname, field in config.fields.items()}

    artifical_dtypes = {name: field.type for name, field in per_pcap_artificial_fields.items()}
    dtypes = dict(tshark_dtypes, **artifical_dtypes)

    # TODO add per_pcap_artificial_fields hash
    pseudohash = hash(config) + hash(frozenset(dtypes.items()))
    uid = cache.cacheuid(
        '',  # prefix (might want to shorten it a bit)
        [filename],  # dependencies
        str(pseudohash) + '.csv'
    )

    is_cache_valid, csv_filename = cache.get(uid)

    log.debug("cache validity=%d cachename: %s" % (is_cache_valid, csv_filename))
    if not is_cache_valid:
        log.info("Cache invalid .. Converting %s " % (filename,))

        with tempfile.NamedTemporaryFile(mode='w+', prefix="mptcpanalyzer-", delete=False) as out:
            tshark_fields = [field.fullname for _, field in config.fields.items()]
            retcode, stderr = config.export_to_csv(filename, out, tshark_fields)
            log.info("exporter exited with code=%d", retcode)
            if retcode is 0:
                out.close()
                cache.put(uid, out.name)
            else:
                raise Exception(stderr)

    log.debug("Loading a csv file %s" % csv_filename)

    try:
        with open(csv_filename) as fd:

            converters = {f.fullname: f.converter for _, f in config.fields.items() if f.converter}
            converters.update({name: f.converter for name, f in per_pcap_artificial_fields.items() if f.converter})

            dtypes = {field.fullname: field.type for _, field in config.fields.items() if field.converter is None}
            log.log(mp.TRACE, "Dtypes before load: %s" % (pp.pformat(dtypes)))
            log.log(mp.TRACE, "Converters before load: %s" % (pp.pformat(converters)))

            # keep this commented code to help diagnosing pandas problems
            # from mptcpanalyzer.debug import read_csv_debug
            # fields = [f.fullname for _, f in config.fields.items()]
            # fields =[ "tcp.options.mptcp.sendkey" ]
            # data = read_csv_debug(fields,

            data = pd.read_csv(
                fd,
                comment='#',
                sep=config.delimiter,
                dtype=dtypes,
                # seems like for now we can't change the default representation apart from converting the column to
                # a string !!!
                # https://stackoverflow.com/questions/46930201/pandas-to-datetime-is-not-formatting-the-datetime-value-in-the-desired-format
                # date_parser=_convert_timestamp,
                # parse_dates=["frame.time_epoch"],
                # ideally DON't user converters but pandas bugs...
                converters=converters,
                # float_precision="high",  # might be necessary
            )

            log.debug("Finished loading CSV file")
            # 1 to 1 -> can't add new columns
            data.rename(inplace=True, columns={f.fullname: name for name, f in config.fields.items()})

            # add new columns
            data = data.assign(**{name: np.nan for name in per_pcap_artificial_fields.keys()})
            column_names = set(data.columns)
            data = data.astype(dtype=artifical_dtypes, copy=False)

            # we want packetid column to survive merges/dataframe transformation so keepit as a column
            # TODO remove ? let other functions do it ?
            data.set_index("packetid", drop=False, inplace=True)
            # log.debug("Column names: %s" % data.columns)

            hashing_fields = [name for name, field in config.fields.items() if field.hash]
            log.debug("Hashing over fields %s" % hashing_fields)

            # won't work because it passes a Serie (mutable)_
            # TODO generate hashing fields from Fields
            # TODO reference stack overflow problem
            temp = pd.DataFrame(data, columns=hashing_fields)
            data["hash"] = temp.apply(lambda x: hash(tuple(x)), axis=1)

    except TypeError as e:
        log.error("You may need to filter more your pcap to keep only mptcp packets")
        raise e
    except Exception as e:
        log.error("You may need to filter more your pcap to keep only mptcp packets")
        raise e

    log.info("Finished loading dataframe for %s. Size=%d" % (input_file, len(data)))

    return data


def pandas_to_csv(df: pd.DataFrame, filename, **kwargs):
    config = mp.get_config()
    return df.to_csv(
        filename,
        sep=config["mptcpanalyzer"]["delimiter"],
        header=True,
        **kwargs
    )



def convert_to_sender_receiver(
    df
    ):
    """
    Convert dataframe from  X_HOST1 | X_HOST2 to X_SENDER | X_RECEIVER

    each packet has a destination marker
    Assume clocks are fine here !
    """
    log.debug("Converting from host_1/host_2 to sender/receiver format")

    # fill up afterwards
    total = pd.DataFrame()

    for tcpstream, subdf in df.groupby(_first("tcpstream")):

        min_h1 = subdf.iloc[0, subdf.columns.get_loc(_first('abstime'))]
        min_h2 = subdf.iloc[0, subdf.columns.get_loc(_second('abstime'))]

#         def _rename_columns(h1_role: ConnectionRoles):
#             """
# client_suffix, server_suffix
#             Params:
#                 client_suffix must be one of HOST1_SUFFIX or HOST2_SUFFIX
#                 server_suffix can be deduced
#             """
        def _rename_column(col_name, suffixes) -> str:

            for suffix_to_replace, new_suffix in suffixes.items():
                if col_name.endswith(suffix_to_replace):
                    return col_name.replace(suffix_to_replace, new_suffix)
            return col_name


            # total = pd.concat([total, subdf], ignore_index=True)

        assert min_h1 != min_h2, ("Same sending and receiving time."
            "Either the clock is not precise enough or it's a bug"
            " (more likely)")
        log.debug("Comparing %f (h1) with %f (h2)" % (min_h1, min_h2))
        if min_h1 < min_h2:
            log.debug("Looks like h1 is the tcp client")
            # suffixes = { HOST1_SUFFIX: SENDER_SUFFIX, HOST2_SUFFIX: RECEIVER_SUFFIX }
            h1_role = ConnectionRoles.Client

        else:
            if min_h1 == min_h2:
                log.warn("there is an issue")

            log.debug("Looks like h2 is the tcp client")
            h1_role = (ConnectionRoles.Server)

        # _rename_columns(role)
        for tcpdest, tdf in subdf.groupby(_first("tcpdest"), sort=False):
            if tcpdest == h1_role:
                suffixes = {HOST2_SUFFIX: SENDER_SUFFIX, HOST1_SUFFIX: RECEIVER_SUFFIX}
            else:
                suffixes = {HOST1_SUFFIX: SENDER_SUFFIX, HOST2_SUFFIX: RECEIVER_SUFFIX}

            log.debug("suffixes: %s" % suffixes)
            rename_func = functools.partial(_rename_column, suffixes=suffixes)
            log.log(mp.TRACE, "renaming inplace")
            rename_func = functools.partial(_rename_column, suffixes=suffixes)

            log.debug("total df size = %d" % len(total))
            with pd.option_context('precision', 20):
                print(tdf[ _first(["abstime", "tcpdest"]) + _second(["abstime", "tcpdest"])] )
                tdf = tdf.rename(columns=rename_func, copy=True, inplace=False)
                print(tdf[ _sender(["abstime", "tcpdest"]) + _receiver(["abstime", "tcpdest"]) ])
                # debug_dataframe(tdf, "temporary dataframe")
                total = pd.concat([total, tdf], ignore_index=True, sort=False, )
                print("total df size = %d" % len(total))

        # subdf[ _first("tcpdest") == ConnectionRole.Client] .rename(columns=_rename_cols, inplace=True)
        # print(subdf.columns)
        # print(total.columns)
    # debug_dataframe(total, "total")

    logging.debug("Converted to sender/receiver format")
    return total


def merge_tcp_dataframes_known_streams(
    con1: Tuple[pd.DataFrame, TcpConnection],
    con2: Tuple[pd.DataFrame, TcpConnection]
    # , dest: ConnectionRoles
) -> pd.DataFrame:
    """
    Generates an intermediate file with the owds.

    1/ clean up dataframe to keep
    2/ identify which dataframe is server's/client's
    2/


    Args:
        con1: Tuple dataframe/tcpstream id
        con2: same

    Returns:
        A dataframe with a "merge_status" column and valid tcp/mptcp destinations

        To ease debug we want to see packets in chronological order

    """
    h1_df, main_connection = con1
    h2_df, mapped_connection = con2

    log.info(
        "Trying to merge connection {} to {} of respective sizes {} and {}".format(
        mapped_connection, main_connection, len(h1_df), len(h2_df)
    ))

    # cleanup the dataframes to contain only the current stream packets
    h1_df = h1_df[h1_df.tcpstream == main_connection.tcpstreamid]
    h2_df = h2_df[h2_df.tcpstream == mapped_connection.tcpstreamid]

    # TODO reorder columns to have packet ids first !
    total = pd.DataFrame()

    for tcpdest in ConnectionRoles:

        log.debug("Merging tcp destination %s" % tcpdest)
        q = main_connection.generate_direction_query(tcpdest)
        h1_unidirectional_df = h1_df.query(q, engine="python")
        q = mapped_connection.generate_direction_query(tcpdest)
        h2_unidirectional_df = h2_df.query(q, engine="python")

        res = map_tcp_packets(h1_unidirectional_df, h2_unidirectional_df)

        # pandas trick to avoid losing dtype
        # see https://github.com/pandas-dev/pandas/issues/22361#issuecomment-413147667
        # no need to set _second (as they are just opposite)
        # TODO this should be done somewhere else
        # else summary won't work
        res[_first('tcpdest')][:] = tcpdest
        res[_second('tcpdest')][:] = tcpdest

        # generate_mptcp_direction_query
        if isinstance(main_connection, MpTcpSubflow):

            log.debug("This is a subflow, setting mptcp destinations...")
            mptcpdest = main_connection.mptcp_dest_from_tcpdest(tcpdest)
            res[_first('mptcpdest')][:] = mptcpdest
            res[_second('mptcpdest')][:] = mptcpdest

            log.debug("Setting mptcpdest to %s" % mptcpdest)

        total = pd.concat([res, total])
        debug_dataframe(total, "concatenated df", usecols=_first(["abstime", "tcpdest"]) + _second(["abstime", "tcpdest"]))

    log.info("Resulting merged tcp dataframe of size {} ({} mapped packets vs {} unmapped)"
            "with input dataframes of size {} and {}.".format(
        len(total),
        len(total[total.merge_status == "both"]), len(total[total.merge_status != "both"]),
        len(h1_df), len(h2_df)
    ))

    # print("unmapped packets:")
    # print(total.loc[total._merge != "both", _sender(TCP_DEBUG_FIELDS) + _receiver(TCP_DEBUG_FIELDS) ])
    return total


# TODO make it part of the api (aka no print) or remove it ?
def merge_mptcp_dataframes(
    df1: pd.DataFrame, df2: pd.DataFrame,
    df1_mptcpstream: MpTcpStreamId
    ) -> Tuple[pd.DataFrame, str]:
    """
    First looks in df2 for a stream matching df1_mptcpstream

    See:
        merge_mptcp_dataframes_known_streams
    """
    main_connection = MpTcpConnection.build_from_dataframe(df1, df1_mptcpstream)

    # we map over df2
    mappings = map_mptcp_connection(df2, main_connection)

    if len(mappings) <= 0:
        # TODO throw instead
        return None, "Could not find a match in the second pcap for mptcpstream %d" % df1_mptcpstream

    if len(mappings) <= 0:
        return None, "Could not find a match in the second pcap for tcpstream %d" % df1_mptcpstream

    print("len(df1)=", len(df1), " len(rawdf2)=", len(df2))
    mapped_connection = mappings[0].mapped
    log.debug("Found mappings %s" % mappings)
    for mapping in mappings:
        print("Con: %s" % (mapping.mapped))

    return merge_mptcp_dataframes_known_streams(
        (df1, main_connection),
        (df2, mapped_connection)
    ), None


def merge_mptcp_dataframes_known_streams(
    con1: Tuple[pd.DataFrame, MpTcpConnection],
    con2: Tuple[pd.DataFrame, MpTcpConnection]
) -> pd.DataFrame:
    """
    Useful for reinjections etc...

    See
        merge_mptcp_dataframes

    Returns:
        Per-subflow dataframes

    """
    df1, main_connection = con1
    df2, mapped_connection = con2

    log.info("Merging %s with %s" % (main_connection, mapped_connection,))

    mapping = map_mptcp_connection_from_known_streams(main_connection, mapped_connection)

    # todo should be inplace
    df_total = None  # type: pd.DataFrame
    for sf, mapped_sf in mapping.subflow_mappings:

        df_temp = merge_tcp_dataframes_known_streams(
            (df1, sf),
            (df2, mapped_sf.mapped)
        )

        df_total = pd.concat([df_temp, df_total])

    # we do it a posteriori so that we can still debug a dataframe with full info
    # print(df_total.columns)
    # cols2drop = [ 'tcpflags']
    # cols2drop = _receiver(cols2drop)
    # df_total.drop(labels=cols2drop)

    return df_total


def map_tcp_packet(df, packet, explain=False) -> List[Tuple[Any, float]]:
    # instead should be index ?
    """
    Packets may disappear, get retransmitted

    Args:
        packet:

    Returns:
        a list of tuples (pktid, score)
    """

    def _get_pktid(row) -> int:
        return row.packetid
    # used to be row.Index when df.set_index("packetid") was in use

    def _cmp_packets(p1, p2) -> float:
        """
        returns a score
        """
        # log.debug("comparing packets %s and %s" % (p1, p2))
        score = 0
        # crude approach, packet with most common fields is declared the best
        # log.debug("comparison based on columns %s " % df.columns)
        for field in df.columns:
            try:
                if explain:
                    log.debug("comparing pktids %d with %d for field %s" % (_get_pktid(packet), _get_pktid(row), field))
                f1 = getattr(p1, field)
                f2 = getattr(p2, field)
                score += scoring_rules[field](f1, f2)
                # log.debug("new score after column [%s] = %f" % (field, score))
                if math.isinf(score):
                    if explain:
                        log.debug("Score set to infinity for field %s" % field)
                    break
            except Exception as e:
                pass
                # log.debug("Exception %s" % str(e))

        # when several packets have same dsn/ack number, we add the difference between
        # absolute times so that the first gets a better score to referee between those
        # score -= abs(p2.abstime - p1.abstime)
        return score

    scores = []  # type: List[Any]

    for row in df.itertuples():

        score = _cmp_packets(packet, row)

        # we don't append inf results for performance reasons
        if not math.isinf(score):
            log.debug("packet %d mapped to %d with a score of %d" % (_get_pktid(packet), _get_pktid(row), score))
            scores.append((_get_pktid(row), score))
        # else:
        #     log.debug("Found no match for pktid %d, skipping.." % _get_pktid(packet))

    # sort by score
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores


def print_weird_owds(df):
    """
    Checks for negative owds
    """
    negative_owds = df[df.owd < 0]
    print("Listing")
    temp = ("row sender pktid={packetid" + SENDER_SUFFIX + "}/abstime={abstime" + SENDER_SUFFIX + "}"
            " pktid={packetid_receiver}/abstime={abstime_receiver} owd={owd}")

    for row in negative_owds.itertuples():
        print(temp.format(**row._asdict()))


def map_tcp_packets(
    host1_df, host2_df,
    explain=[],
    mode="hash"
    # con1: TcpConnection, con2: TcpConnection
) -> pd.DataFrame:
    '''
    Dataframe with format
    '''
    if mode == "hash":
        res = map_tcp_packets_via_hash(host1_df, host2_df, explain)
    else:
        res = map_tcp_packets_score_based(host1_df, host2_df, explain)

    log.info("Merged dataframe of size {} generated from {} and {} sources.".format(
        len(res), len(host1_df), len(host2_df)
    ))
    log.info("{} unmapped packets. ".format(
        len(res[res.merge_status == "left_only"]) + len(res[res.merge_status == "right_only"])
    ))

    def _show_unmapped_pkts():
        print(res[res.merge_status == "left_only"])
        print(res[res.merge_status == "right_only"])

    _show_unmapped_pkts()

    return res

def deal_with_duplicated_hash(df):
    '''
    We tell pandas to asser for duplicated packets, thus we need to deal
    with them ourselves
    '''
    log.info("looking for duplicate hashes/packets)")
    dups = df["hash"].duplicated()
    nb_dups = len(df.loc[dups, :])
    # TODO should count the number of duplicated packets
    log.debug("Got %d duplicated hash" % nb_dups)
    # if we had duplicated print them
    if nb_dups > 0:

        log.debug(df.loc[dups, :])
        log.warn("Dropping duplicates in ")
        df.drop_duplicates(
            subset=["hash"],
            inplace=True,
            # TODO maybe we should make it interactive ? or configurable
            keep="first",
        )
    return df


def map_tcp_packets_via_hash(
    # TODO rename, these are not host1/host2 anymore
    host1_df, host2_df,
    *kargs, **kwargs
    ):
    """
    Merge on hash of different fields
    Resulting dataframe has H1_SUFFIX / H2_SUFFIX
    """
    log.info("Merging dataframes via hash")
    debug_cols = ["packetid", "hash", "abstime"]
    # debug_dataframe(total, "concatenated df", usecols=_first(["abstime", "tcpdest"]) + _second(["abstime", "tcpdest"]))

    debug_dataframe(host1_df, "host1_df", )
    debug_dataframe(host2_df, "host2 df")
    # print("host1_df dtype=", host1_df.dtypes.tcpdest)
    # print("host2_df dtype=", host2_df.dtypes.tcpdest)

    # todo we could now use merge_asof
    # TODO here we should be able to drop some columns in double
    try:
        # first check hashes are identical
        # check hashes are different



        host1_df = deal_with_duplicated_hash(host1_df)
        host2_df = deal_with_duplicated_hash(host2_df)

        res = pd.merge(
            host1_df, host2_df,
            on="hash",
            suffixes=(HOST1_SUFFIX, HOST2_SUFFIX),  # columns suffixes
            how="outer", # we want to keep packets from both
            # we want to know how many packets were not mapped correctly, adds the merge column
            # can take values "left_only"/ "right_only" or both
            indicator="merge_status",
            # run additionnal checks against duplicate hashes
            validate="one_to_one",  # can slow process
        )


    except pd.errors.MergeError as e:

        # TODO we don't want to print here
        print("An error happened during the merge of the 2 pcaps")
        print(e)
        raise e


    # TCP_DEBUG_FIELDS
    debug_cols = _first(TCP_DEBUG_FIELDS) + _second(TCP_DEBUG_FIELDS)
    debug_dataframe(res, "Result of merging by hash", usecols=debug_cols)
    return res


def map_tcp_packets_score_based(
    sender_df, receiver_df,
    explain=[],
    mode="hash"
        # con1: TcpConnection, con2: TcpConnection
        ) -> pd.DataFrame:
    """
    Stream ids must already mapped
    Args:
        explain = increase verbosity for packet ids in this list

    Todo:
        check N = len(sender_df) - len(receiver_df) to know how many packets should be missing,
        then cut off lowest N.

    algo:
        Computes a score on a per packet basis
        Based

    Returns:
        a copy of sender_df with as Index the packetid + a new column called
        "mapped_index" matching the Index of rawdf2
        'rcv_pktid', 'score'
    """
    # DataFrame.add(other, axis='columns', level=None, fill_value=None)
    # adds a new column that contains only nan
    log.debug("Mapping TCP packets between TODO")

    # returns a new object with new columns rcv_pktid, score initialized to NaN
    df_final = sender_df.assign(rcv_pktid=np.nan, score=np.nan,)

    # Problem is to identify lost packets and retransmitted ones
    # so that they map to the same or none ?
    limit = 5  # limit nb of scores to display

    pkt_column = df_final.columns.get_loc('rcv_pktid')
    score_column = df_final.columns.get_loc('score')

    for row_id, row in enumerate(sender_df.itertuples(index=False)):

        explain_pkt = row.packetid in explain
        scores = map_tcp_packet(receiver_df, row, explain_pkt)
        # takes best score index
        # df_final.loc[row.index , 'mapped_index'] = 2 # scores[0][0]
        # print(type(row.Index), type(row.index))
        if len(scores) >= 1:
            print("first %d packets (pandas.index/score)s=\n%s" % (limit, scores[:limit]))
            if explain_pkt:
                for idx, score in scores:
                    log.debug("Score %s=%s" % (idx, score))
            idx, score = scores[0]

            # You should never modify something you are iterating over.
            # print("row.Index=%r (out of %d) and idx=%r" % (row.Index, len(df_final), idx))
            print("row.id=%r (out of %d) and idx=%r" % (row_id, len(df_final), idx))
            # print("df_final.=%r " % (row.Index, idx))
            # df_final.iat[row.Index, df_final.columns.get_loc('rcv_pktid')] = idx
            df_final.iloc[row_id, pkt_column] = idx
            # df_final.loc[row.Index, 'rcv_pktid'] = idx
            # df_final.at[row.Index, 'rcv_pktid'] = idx
            # iat only accepts an integer while iloc can accept a tuple etc
            # print(df_final.iat[row.Index].score)
            df_final.iloc[row_id, score_column] = score
            # TODO we might want to remove that packets from further search

        # drop the chosen index so that it doesn't get used a second time
        # todo pb la c qu'on utilise les packet id comme des index :/
            print("Score %f assigned to index %s" % (score, idx))
            # df2.drop(df2.index[[idx]], inplace=True)
            # df2.drop(idx, inplace=True)
        else:
            # log.debug("No map found for this packet")
            log.debug("Found no match for pktid %d, skipping.." % row.packetid)

        # print("registered = %s" % ( df_final.loc[row.Index, 'mapped_index'])) # , ' at index: ', row.index )

    # checks that
    # if df_final.rcv_pktid.is_unique is False:
    #     log.warn("There seems to be an error: some packets were mapped several times.")

    # print("head=\n", df_final.head())
    return df_final


def map_tcp_stream(rawdf: pd.DataFrame, main: TcpConnection) -> List[TcpMapping]:
    """
    Returns:
        a sorted list of mappings (tcpconnection, score) with the first one being the most probable
    """

    results = []
    for tcpstream in rawdf["tcpstream"].unique():
        other = TcpConnection.build_from_dataframe(rawdf, tcpstream)
        score = main.score(other)
        if score > float('-inf'):
            mapping = TcpMapping(other, score)
            results.append(mapping)

    # decreasing sort based on the score
    results.sort(key=lambda x: x[1], reverse=True)

    return results

def map_mptcp_connection_from_known_streams(
    # rawdf2: pd.DataFrame,
    main: MpTcpConnection,
    other: MpTcpConnection
    ) -> MpTcpMapping:
    """
    Attempts to map subflows only if score is high enough
    """
    # other = MpTcpConnection.build_from_dataframe(rawdf2, mptcpstream2)
    def _map_subflows(main: MpTcpConnection, mapped: MpTcpConnection):
        """
        """
        mapped_subflows = []
        for sf in main.subflows():

            # generates a list (subflow, score)
            scores = list(map(lambda x: TcpMapping(x, sf.score(x)), mapped.subflows()))
            scores.sort(key=lambda x: x[1], reverse=True)
            log.log(mp.TRACE, "sorted scores when mapping %s:\n %r" % (sf, scores))
            mapped_subflows.append((sf, scores[0]))
        return mapped_subflows

    mptcpscore = main.score(other)
    mapped_subflows = None
    if mptcpscore > float('-inf'):
        # (other, score)
        mapped_subflows = _map_subflows(main, other)

    mapping = MpTcpMapping(mapped=other, score=mptcpscore, subflow_mappings=mapped_subflows)
    log.log(mp.TRACE, "mptcp mapping %s" % (mapping,))
    return mapping


def map_mptcp_connection(
    rawdf2: pd.DataFrame, main: MpTcpConnection
    ) -> List[MpTcpMapping]:
    """
    warn: Do not trust the results yet WIP !

    Returns:
        List of (connection, score) with the best mapping first

    This function tries to map a mptcp.stream from a dataframe (aka pcap) to mptcp.stream
    in another dataframe. For now it just looks at IP level stuff without considering subflow
    mapping score
    """
    log.warning("mapping between datasets is not considered trustable yet")
    results = []  # type: List[MpTcpMapping]

    score = -1  # type: float
    results = []

    for mptcpstream2 in rawdf2[_sender("mptcpstream")].dropna().unique():
        other = MpTcpConnection.build_from_dataframe(rawdf2, mptcpstream2)
        mapping = map_mptcp_connection_from_known_streams(main, other)
        results.append(mapping)

    # sort based on the score
    results.sort(key=lambda x: x[1], reverse=True)

    return results


def classify_reinjections(df_all: pd.DataFrame) -> pd.DataFrame:
    """
    look at reinjections on the receiver side, see which one is first
    packets with reinjected_in_receiver are (at least they should) be the first DSN arrived.

    Returns:
        a new dataframe with an added column "redundant" and "time_delta"
    """
    log.debug("Classifying reinjections")

    # use assign
    df_all = df_all.assign(redundant=False, reinj_delta=np.nan)

    df = df_all[df_all.merge_status == "both"]

    # print(df_all[ pd.notnull(df_all[_sender("reinjection_of")])] [
    #     _sender(["reinjection_of", "reinjected_in", "packetid", "reltime"]) +
    #     _receiver(["packetid", "reltime"])
    # ])

    for destination in ConnectionRoles:

        log.debug("Looking at mptcp destination %r" % destination)
        sender_df = df[df.mptcpdest == destination]

        # print(sender_df[ sender_df.reinjected_in.notna() ][["packetid", "reinjected_in"]])
        # print("successful reinjections" % len(reinjected_in))

        # select only packets that have been reinjected

        # print("%d sender_df packets" % len(sender_df))
        # print(sender_df["reinjection_of"])
        reinjected_packets = sender_df.dropna(axis='index', subset=[_sender("reinjection_of")])

        log.debug("%d reinjected packets" % len(reinjected_packets))
        # with pd.option_context('display.max_rows', None, 'display.max_columns', 300):
        #     print(reinjected_packets[
        #         _sender(["packetid", "reinjected_in", "reinjection_of"]) + _receiver(["reinjected_in", "reinjection_of"])
        #         ].head())


        for reinjection in reinjected_packets.itertuples():
            # here we look at all the reinjected packets

            # print("full reinjection %r" % (reinjection,))

            # if there are packets in _receiver(reinjected_in), it means the reinjections
            # arrived before other similar segments and thus these segments are useless
            # it should work because
            # useless_reinjections = getattr(reinjection, _receiver("reinjected_in"), [])

            # if it was correctly mapped
            # TODO why reinjection._merge doesn't exist ?
            if reinjection.merge_status != "both":
                # TODO count missed classifications ?
                log.log(mp.TRACE, "reinjection %d could not be mapped, giving up..." % (reinjection.packetid))
                continue

            # print("%r" % reinjection.reinjection_of)
            initial_packetid = reinjection.reinjection_of[0]
            # print("initial_packetid = %r %s" % (initial_packetid, type(initial_packetid)))

            original_packet = df_all.loc[df_all.packetid == initial_packetid].iloc[0]

            if original_packet.merge_status != "both":
                # TODO count missed classifications ?
                log.log(mp.TRACE, "Original packet %d could not be mapped, giving up..." % (original_packet.packetid))
                continue

            orig_arrival = getattr(original_packet, _receiver("reltime"))
            reinj_arrival = getattr(reinjection, _receiver("reltime"))
            reinj_pktid = getattr(reinjection, _sender("packetid"))

            reinj_delta = orig_arrival - reinj_arrival
            df_all.loc[reinj_pktid, "reinj_delta"] = reinj_delta

            if reinj_delta < 0:
                # print("GOT A MATCH")
                df_all.loc[df_all[_sender("packetid")] == reinjection.packetid, "redundant"] = True
                #TODO set reinj_delta for reinjection.packetid

    return df_all
