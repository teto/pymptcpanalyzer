import logging
import os
import pandas as pd
import numpy as np
from mptcpanalyzer.tshark import TsharkConfig, Field, FieldDate
from mptcpanalyzer.connection import MpTcpSubflow, MpTcpConnection, TcpConnection, \
    MpTcpMapping, TcpMapping, swap_role, TcpStreamId, MpTcpStreamId
import mptcpanalyzer as mp
from mptcpanalyzer import (RECEIVER_SUFFIX, SENDER_SUFFIX, _receiver, _sender,
HOST1_SUFFIX, HOST2_SUFFIX,
_first, _second, ConnectionRoles)
from typing import List, Any, Tuple, Dict, Callable, Union
import math
import tempfile
import pprint
from enum import Enum, auto
import functools
import mptcpanalyzer.debug
from mptcpanalyzer.debug import debug_dataframe

log = logging.getLogger(__name__)

pp = pprint.PrettyPrinter(indent=4)


# TODO might need a converter when saving/loading
# TODO pandas.api.types.register_extension_dtype()
# the list here is important in fact
dtype_role = pd.api.types.CategoricalDtype(categories=list(ConnectionRoles), ordered=True)

TCP_DEBUG_FIELDS = ['packetid', "reltime", "abstime"]
MPTCP_DEBUG_FIELDS = TCP_DEBUG_FIELDS + ['mptcpdest']


def _convert_role(x):
    """
    Workaround https://github.com/pandas-dev/pandas/pull/20826
    """
    log.log(mp.TRACE, "converting [%r] into role", x)
    return ConnectionRoles(x)

def ignore(f1, f2):
    return 0

def exact(f1, f2):
    return 10 if (math.isnan(f1) and math.isnan(f2)) or f1 == f2 else float('-inf')

def diff(f1, f2):
    return f2 - f1

def debug_convert(df):
    return df
    # return df.head(20)


def getrealpath(input_file):
    filename = os.path.expanduser(input_file)
    filename = os.path.realpath(filename)
    return filename


"""
when trying to map packets from a pcap to another, we give a score to each mapping
based on per-field rules.

invariant: True if not modified by the network
Of the form Field.shortname

Have a look at the graphic slide 28:
https://www-phare.lip6.fr/cloudnet12/Multipath-TCP-tutorial-cloudnet.pptx

"""
scoring_rules = {
    "packetid": ignore,
    # in-order packets are more common than out of order ones
    "abstime": diff,
    "default_time": ignore,
    "expected_token": exact,
    "sport": exact,
    "dport": exact,
    "rwnd": exact,
    "sendkey": exact,
    "rcvkey": exact,
    "rcvtoken": exact,
    "tcpflags": exact,
    "dss_dsn": exact,
    "dss_rawack": exact,
    "dss_ssn": exact,
    "tcpseq": exact,
    "tcplen": exact,
}

"""
On top of Tshark fields, we also describe fields generated by mptcpanalyzer
"""
per_pcap_artificial_fields = {
    "mptcpdest": Field("mptcpdest", dtype_role, "MPTCP destination", False, None),
    "tcpdest": Field("tcpdest", dtype_role, "TCP destination", False, None),

    # "mptcpdest": Field("mptcpdest", None, "MPTCP destination", False, _convert_role),
    # "tcpdest": Field("tcpdest", None, "TCP destination", False, _convert_role),

    # TODO use int? as type
    "hash": Field("hash", str, "Hash of fields", False, None),

    # TODO rename ?
    # TODO should be a CategoryDataType !
    # "merge": Field("_merge", None, "How many packets were merged", False, None)
}

FieldList = Dict[str, Field]

def get_date_cols(fields: FieldList):
    return [name for name, f in fields.items() if isinstance(f, FieldDate)]

def get_converters(fields: FieldList):
    return {name: f.converter for name, f in fields.items() if f.converter}

def get_dtypes(fields: FieldList):
    return {name: field.type for name,
                field in fields.items() if field.converter is None}

def date_converter(x):
    return pd.to_datetime(x, unit="s", utc=True)

class PacketMappingMode(Enum):
    """
    How to map packets from one stream to another

    The SCORE based algorithm allows to work with traffic that went trhoug NATs
    etc but is buggy/less tested

    The hash based is more straightforward
    """
    HASH = auto()
    SCORE = auto()


def check_df(f, checks):
    '''
    decorator checking that dataframe fulfill some conditions
    first argument (dataframe)
    '''
    # TODO
    @functools.wraps(f)
    def wrapped(self, *args):
        if self.data is not None:
            return f(self, *args)
        else:
            raise mp.MpTcpException("Please load a pcap with `load_pcap` first")
        return None
    return wrapped


def drop_syn(df: pd.DataFrame, mptcp: bool = True) -> pd.DataFrame:
    """
    for mptcp it's as easy as removing packets with MP_CAPABLE or MP_JOIN
    """

    # syns = df[df.tcpflags == mp.TcpFlags.SYN]

    syn_index = df.tcpflags.where(lambda x: x & mp.TcpFlags.SYN).dropna()
    return df.drop(syn_index.Index)


def load_merged_streams_into_pandas(
    pcap1: str,
    pcap2: str,
    streamid1: int,
    streamid2: int,
    # TODO changed to protocol
    mptcp: bool,
    tshark_config: TsharkConfig,
    clock_offset1: int = 0,
    clock_offset2: int = 0,
    mapping_mode: PacketMappingMode = PacketMappingMode.HASH,
    **extra
):
    """
    Arguments:
        protocol: mptcp or tcp
        mapping_mode: Only HASH works for now
        clock_offset: untested

    Returns
        a dataframe with columns... owd ?
    """
    protocolStr = "mptcp" if mptcp else "tcp"
    log.debug(f"Asked to load {protocolStr} merged streams {streamid1} and "
        "{streamid2} from pcaps {pcap1} and {pcap2}"
              )

    cache = mp.get_cache()

    cacheid = cache.cacheuid("merged", [getrealpath(pcap1), getrealpath(pcap2)],
        protocolStr + "_" + str(streamid1) + "_" + str(streamid2) + ".csv")

    # if we can't load that file from cache
    try:
        merged_df = pd.DataFrame()
        res = pd.DataFrame()

        valid, cachename = cache.get(cacheid)
        log.info("Cache validity=%s and cachename=%s" % (valid, cachename))

        # TODO disable when clock_offset is set
        if not valid:
            df1 = load_into_pandas(pcap1, tshark_config, clock_offset=clock_offset1)
            df2 = load_into_pandas(pcap2, tshark_config, clock_offset=clock_offset2)

            main_connection = None  # type: Union[MpTcpConnection, TcpConnection]
            other_connection = None  # type: Union[MpTcpConnection, TcpConnection]
            if mptcp:
                main_connection = MpTcpConnection.build_from_dataframe(
                    df1, MpTcpStreamId(streamid1))
                other_connection = MpTcpConnection.build_from_dataframe(
                    df2, MpTcpStreamId(streamid2))

                # for now we use known streams exclusively
                # might be interested to use merge_tcp_dataframes later
                merged_df = merge_mptcp_dataframes_known_streams(
                    (df1, main_connection),
                    (df2, other_connection)
                )

            else:
                main_connection = TcpConnection.build_from_dataframe(df1, TcpStreamId(streamid1))
                other_connection = TcpConnection.build_from_dataframe(df2, TcpStreamId(streamid2))

                # for now we use known streams exclusively
                # might be interested to use merge_tcp_dataframes later
                merged_df = merge_tcp_dataframes_known_streams(
                    (df1, main_connection),
                    (df2, other_connection)
                )

            assert cachename
            log.info("Saving into %s" % cachename)
            # trying to export lists correctly
            # print(merged_df.reinjected_in.dropna().head())
            # convert arrays back to strings
            # merged_df.apply(",".join()
            # or abstime ?

            # TODO rechange the flags hex()
            merged_df.to_csv(
                cachename,
                # columns=columns,
                index=False,
                header=True,
                sep=tshark_config.delimiter,
            )

            # tcpdest had become an objected instead of a CategoricalDtype
            # see https://github.com/pandas-dev/pandas/issues/22361
            log.log(mp.TRACE, "saving with dtypes=", dict(merged_df.dtypes))

        else:
            log.info("Loading from cache %s", cachename)

            date_cols = get_date_cols(tshark_config.fields)

            with open(cachename) as fd:
                # generate fieldlist
                def _gen_fields(fields):
                    gfields = {}  # type: ignore
                    for _name in [_first, _second]:
                        gfields.update({_name(k): v for k, v in fields.items()})
                    return gfields

                # reltime discarded on save ?
                tshark_config.fields.pop("reltime")
                gfields = _gen_fields(tshark_config.fields)
                merge_dtypes = get_dtypes(gfields)
                # log.log(mp.TRACE, "Using gfields %s" % pp.pformat(gfields))

                # we don't need any converters
                converters = {}
                date_cols = get_date_cols(gfields)

                log.log(mp.TRACE, "Using date_cols %s" % pp.pformat(date_cols))
                log.log(mp.TRACE, "Using dtypes %s" % pp.pformat(merge_dtypes))
                # log.log(mp.TRACE, "Using converters %s" % (pp.pformat(converters)))
                merged_df = pd.read_csv(
                    fd,
                    skip_blank_lines=True,
                    comment='#',
                    # we don't need 'header' when metadata is with comment
                    sep=tshark_config.delimiter,
                    # memory_map=True, #Â could speed up processing
                    dtype=merge_dtypes,  # poping still generates
                    converters=converters,
                    # date_parser=date_converter,
                    parse_dates=date_cols,
                )
                # at this stage, destinatiosn are nan

                debug_fields = ["abstime", "tcpstream", "tcpdest", "mptcpdest"]
                mptcpanalyzer.debug.debug_dataframe(merged_df, "Merged dataframe",
                    usecols=(_first(debug_fields) + _second(debug_fields)))

                # workaround bug https://github.com/pandas-dev/pandas/issues/25448
                def _convert_to_enums():
                    # per_pcap_artificial_fields
                    for col in [_first("tcpdest"), _first("mptcpdest"), _second("tcpdest"), _second("mptcpdest")]:
                        merged_df[col] = merged_df[col].apply(_convert_role, convert_dtype=False)

        # we fix the clocks a posteriori so that the cache is still usable
        log.debug("Postprocessing clock if needed")
        # merged_df[_first('abstime')] += clock_offset1
        # merged_df[_second('abstime')] += clock_offset2

        log.debug("Converting dataframes to be sender/receiver based...")

        # in both cases
        # TODO here we should attribute the definite mptcprole
        if mptcp:
            log.error("We should correct the clocks if the argument is passed !")
            # raise mp.MpTcpException("Implement mptcp merge")

            res = convert_to_sender_receiver(merged_df)
            # fill MPTCP dest ?
        else:
            # tcp
            res = convert_to_sender_receiver(merged_df)

        # log.debug("Sorting by sender abstime")
        # merged_df.sort_values(by=_sender("abstime"), ascending=True, inplace=True)
        # debug_dataframe(res, "checking merge", usecols=["merge_status"])
        # print("%d nan values" % len(res[res.merge_status == np.nan]))

        log.debug("Computing owds")

        debug_dataframe(res, "before owds")
        # TODO we don't necessarely need to generate the OWDs here, might be put out
        res['owd'] = res[_receiver('abstime')] - res[_sender('abstime')]

        debug_dataframe(
            res, "owd", usecols=["owd", _sender('abstime'), _receiver('abstime')]
        )
        # with pd.option_context('float_format', '{:f}'.format):
        #     print(
        #         res[_sender(["ipsrc", "ipdst", "abstime"])
        #          + _receiver(["abstime", "packetid"]) + TCP_DEBUG_FIELDS + ["owd"] ]
        #     )

    except Exception as e:
        log.exception("exception happened while merging")

    # pd.set_option('display.max_rows', 200)
    # pd.set_option('display.max_colwidth', -1)
    # print("dtypes=", dict(dtypes))
    log.log(mp.TRACE, "Dtypes after load:%s\n", pp.pformat(res.dtypes))
    log.info("Finished loading. merged dataframe size: %d", len(res))

    return res


def load_into_pandas(
    input_file: str,
    config: TsharkConfig,
    **extra
) -> pd.DataFrame:
    """
    load mptcp data into pandas

    Args:
        input_file: pcap filename
        config: Hard, keep changing
        load_cb: callback to use if cache not available
        extra: extra arguments to forward to load_cb
    """
    log.debug("Asked to load simple pcap %s", input_file)

    filename = getrealpath(input_file)
    cache = mp.get_cache()

    # {fullname: field.type for fullname, field in config.fields.items()}
    tshark_dtypes = get_dtypes(config.fields)

    artifical_dtypes = get_dtypes(per_pcap_artificial_fields)
    dtypes = dict(tshark_dtypes, **artifical_dtypes)

    # TODO add per_pcap_artificial_fields hash
    pseudohash = hash(config) + hash(frozenset(dtypes.items()))
    uid = cache.cacheuid(
        '',  # prefix (might want to shorten it a bit)
        [filename],  # dependencies
        str(pseudohash) + '.csv'
    )
    # print(config.fields)

    is_cache_valid, csv_filename = cache.get(uid)

    log.debug("cache validity=%d cachename: %s", is_cache_valid, csv_filename)
    if not is_cache_valid:
        log.info("Cache invalid .. Converting %s", filename,)

        with tempfile.NamedTemporaryFile(mode='w+', prefix="mptcpanalyzer-", delete=False) as out:
            # tshark_fields = [field.fullname for _, field in config.fields.items()]
            tshark_fields = {field.fullname: name for name, field in config.fields.items()}
            retcode, _, stderr = config.export_to_csv(filename, out, tshark_fields)
            log.info("exporter exited with code=%d", retcode)
            if retcode == 0:
                out.close()
                cache.put(uid, out.name)
            else:
                raise Exception(stderr)

    log.debug("Loading a csv file %s", csv_filename)

    try:
        with open(csv_filename) as fd:

            # gets a list of fields to convert
            # we dont want to modify the passed parameter
            fields = config.fields.copy()
            fields.update(per_pcap_artificial_fields)
            converters = get_converters(config.fields)

            # builds a list of fields to be parsed as dates
            # (since converter/types don't seem to be great)
            date_cols = get_date_cols(config.fields)

            dtypes = get_dtypes(config.fields)

            log.log(mp.TRACE, "Dtypes before load:\n%s", pp.pformat(dtypes))
            log.log(mp.TRACE, "Converters before load:\n%s", pp.pformat(converters))
            log.log(mp.TRACE, "Fields to load as times:\n%s", pp.pformat(date_cols))

            # keep this commented code to help diagnosing pandas problems
            # from mptcpanalyzer.debug import read_csv_debug
            fields = [f.fullname for _, f in config.fields.items()]
            # fields =[ "tcp.options.mptcp.sendkey" ]
            # data = mptcpanalyzer.debug.read_csv_debug(fields,
            data = pd.read_csv(
                fd,
                comment='#',
                sep=config.delimiter,
                dtype=dtypes,
                date_parser=date_converter,
                parse_dates=date_cols,
                # ideally DON't user converters but pandas bugs...
                converters=converters,
                # float_precision="high",  # might be necessary
            )

            log.debug("Finished loading CSV file")
            # 1 to 1 -> can't add new columns
            data.rename(inplace=True, columns={
                        f.fullname: name for name, f in config.fields.items()})

            # add new columns
            data = data.assign(**{name: np.nan for name in per_pcap_artificial_fields.keys()})
            column_names = set(data.columns)
            data = data.astype(dtype=artifical_dtypes, copy=False)

            # we want packetid column to survive merges/dataframe transformation
            # so keepit as a column
            # TODO remove ? let other functions do it ?
            data.set_index("packetid", drop=False, inplace=True)

            hashing_fields = [name for name, field in config.fields.items() if field.hash]
            log.debug("Hashing over fields %s", hashing_fields)

            # won't work because it passes a Serie (mutable)_
            # TODO generate hashing fields from Fields
            # TODO reference stack overflow problem
            temp = pd.DataFrame(data, columns=hashing_fields)
            data["hash"] = temp.apply(lambda x: hash(tuple(x)), axis=1)

    except TypeError as e:
        log.error("You may need to filter more your pcap to keep only mptcp packets")
        raise e
    except Exception as e:
        log.error("You may need to filter more your pcap to keep only mptcp packets")
        raise e

    log.info("Finished loading dataframe for %s. Size=%d", input_file, len(data))

    return data


def pandas_to_csv(df: pd.DataFrame, filename, **kwargs):
    config = mp.get_config()
    return df.to_csv(
        filename,
        sep=config["mptcpanalyzer"]["delimiter"],
        header=True,
        **kwargs
    )


def convert_to_sender_receiver(df) -> pd.DataFrame:
    """
    Convert dataframe from  X_HOST1 | X_HOST2 to X_SENDER | X_RECEIVER

    each packet has a destination marker
    Assume clocks are fine here !
    """
    log.debug("Converting from host_1/host_2 to sender/receiver format")

    # fill up afterwards
    total = pd.DataFrame()

    for tcpstream, subdf in df.groupby(_first("tcpstream")):

        min_h1 = subdf.iloc[0, subdf.columns.get_loc(_first('abstime'))]
        min_h2 = subdf.iloc[0, subdf.columns.get_loc(_second('abstime'))]

#         def _rename_columns(h1_role: ConnectionRoles):
#             """
# client_suffix, server_suffix
#             Params:
#                 client_suffix must be one of HOST1_SUFFIX or HOST2_SUFFIX
#                 server_suffix can be deduced
#             """
        def _rename_column(col_name, suffixes) -> str:

            for suffix_to_replace, new_suffix in suffixes.items():
                if col_name.endswith(suffix_to_replace):
                    return col_name.replace(suffix_to_replace, new_suffix)
            return col_name

            # total = pd.concat([total, subdf], ignore_index=True)
        log.debug(f"Comparing {min_h1} (h1) with {min_h2} (h2)")

        assert min_h1 != min_h2, (f"Same sending {min_h1} and receiving time {min_h2}."
            "Either the clock is not precise enough or it's a bug"
            " (more likely)")
        if min_h1 < min_h2:
            log.debug("Looks like h1 is the tcp client")
            # suffixes = { HOST1_SUFFIX: SENDER_SUFFIX, HOST2_SUFFIX: RECEIVER_SUFFIX }
            h1_role = ConnectionRoles.Client

        else:
            if min_h1 == min_h2:
                log.warn("there is an issue")

            log.debug("Looks like h2 is the tcp client")
            h1_role = (ConnectionRoles.Server)

        # _rename_columns(role)
        for tcpdest, tdf in subdf.groupby(_first("tcpdest"), sort=False):
            if tcpdest == h1_role:
                suffixes = {HOST2_SUFFIX: SENDER_SUFFIX, HOST1_SUFFIX: RECEIVER_SUFFIX}
            else:
                suffixes = {HOST1_SUFFIX: SENDER_SUFFIX, HOST2_SUFFIX: RECEIVER_SUFFIX}

            log.debug("suffixes: %s" % suffixes)
            rename_func = functools.partial(_rename_column, suffixes=suffixes)
            log.log(mp.TRACE, "renaming inplace")
            rename_func = functools.partial(_rename_column, suffixes=suffixes)

            log.debug("total df size = %d" % len(total))
            with pd.option_context('precision', 20):
                debug_cols = _first(["abstime", "tcpdest"]) + _second(["abstime", "tcpdest"])
                log.log(mp.TRACE, "before rename \n%s", tdf[debug_cols])
                tdf = tdf.rename(columns=rename_func, copy=True, inplace=False)

                debug_cols = _sender(["abstime", "tcpdest"]) + _receiver(["abstime", "tcpdest"])
                log.log(mp.TRACE, "After rename \n%s" % tdf[debug_cols])
                # print(tdf[debug_cols])
                # debug_dataframe(tdf, "temporary dataframe")
                total = pd.concat([total, tdf], ignore_index=True, sort=False, )
                # print("total df size = %d" % len(total))

        # subdf[ _first("tcpdest") == ConnectionRole.Client] .rename(columns=_rename_cols, inplace=True)
        # print(subdf.columns)
        # print(total.columns)
    # debug_dataframe(total, "total")

    log.debug("Converted to sender/receiver format")
    log.log(mp.TRACE, "Comparing #unique entries %d vs #all %d",
            total[_sender("abstime")].count(), len(total[_sender("abstime")]))
    # assert total[_sender("abstime")].count() == len(total[_sender("abstime")])
    return total


def merge_tcp_dataframes_known_streams(
    con1: Tuple[pd.DataFrame, TcpConnection],
    con2: Tuple[pd.DataFrame, TcpConnection]
) -> pd.DataFrame:
    """
    Generates an intermediate file with the owds.

    1/ clean up dataframe to keep
    2/ identify which dataframe is server's/client's

    Args:
        con1: Tuple dataframe/tcpstream id
        con2: same

    Returns:
        A dataframe with a "merge_status" column and valid tcp/mptcp destinations
        To ease debug we want to see packets in chronological order
    """
    h1_df, main_connection = con1
    h2_df, mapped_connection = con2

    log.info(
        "Trying to merge connection {} to {} of respective sizes {} and {}".format(
            mapped_connection, main_connection, len(h1_df), len(h2_df)
        )
    )

    # cleanup the dataframes to contain only the current stream packets
    h1_df = h1_df[h1_df.tcpstream == main_connection.tcpstreamid]
    h2_df = h2_df[h2_df.tcpstream == mapped_connection.tcpstreamid]

    # TODO reorder columns to have packet ids first !
    total = pd.DataFrame()

    for tcpdest in ConnectionRoles:

        log.debug("Merging tcp destination %s" % tcpdest)
        q = main_connection.generate_direction_query(tcpdest)
        h1_unidirectional_df = h1_df.query(q, engine="python")
        q = mapped_connection.generate_direction_query(tcpdest)
        h2_unidirectional_df = h2_df.query(q, engine="python")

        res = map_tcp_packets(h1_unidirectional_df, h2_unidirectional_df)

        # pandas trick to avoid losing dtype
        # see https://github.com/pandas-dev/pandas/issues/22361#issuecomment-413147667
        # no need to set _second (as they are just opposite)
        # TODO this should be done somewhere else
        # else summary won't work
        res[_first('tcpdest')][:] = tcpdest
        res[_second('tcpdest')][:] = tcpdest

        # generate_mptcp_direction_query
        # TODO this is not always reached ?
        log.info("con of TYPE %r", main_connection)
        if isinstance(main_connection, MpTcpSubflow):

            log.debug("This is a subflow, setting mptcp destinations...")
            mptcpdest = main_connection.mptcp_dest_from_tcpdest(tcpdest)
            log.debug("Setting mptcpdest to {mptcpdest}")
            res[_first('mptcpdest')][:] = mptcpdest
            res[_second('mptcpdest')][:] = mptcpdest

            log.debug("Setting mptcpdest to %s" % mptcpdest)

        total = pd.concat([res, total])
        debugcols = _first(["abstime", "tcpdest", "mptcpdest"]) + \
        _second(["abstime", "tcpdest", "mptcpdest"])
        debug_dataframe(total, "concatenated df", usecols=debugcols)

    log.info("Resulting merged tcp dataframe of size {} ({} mapped packets vs {} unmapped)"
            "with input dataframes of size {} and {}.".format(
                len(total),
                len(total[total.merge_status == "both"]), len(total[total.merge_status != "both"]),
                len(h1_df), len(h2_df)
            )
    )

    # print("unmapped packets:")
    # print(total.loc[total._merge != "both", _sender(TCP_DEBUG_FIELDS) + _receiver(TCP_DEBUG_FIELDS) ])
    return total


# TODO make it part of the api (aka no print) or remove it ?
def merge_mptcp_dataframes(
    df1: pd.DataFrame, df2: pd.DataFrame,
    df1_mptcpstream: MpTcpStreamId
) -> Tuple[pd.DataFrame, str]:
    """
    First looks in df2 for a stream matching df1_mptcpstream

    See:
        merge_mptcp_dataframes_known_streams
    """
    main_connection = MpTcpConnection.build_from_dataframe(df1, df1_mptcpstream)

    # we map over df2
    mappings = map_mptcp_connection(df2, main_connection)

    if len(mappings) <= 0:
        # TODO throw instead
        return None, "Could not find a match in the second pcap for mptcpstream %d" % df1_mptcpstream

    if len(mappings) <= 0:
        return None, "Could not find a match in the second pcap for tcpstream %d" % df1_mptcpstream

    # print("len(df1)=", len(df1), " len(rawdf2)=", len(df2))
    mapped_connection = mappings[0].mapped
    log.debug("Found mappings %s" % mappings)
    for mapping in mappings:
        log.debug("Con: %s" % (mapping.mapped))

    return merge_mptcp_dataframes_known_streams(
        (df1, main_connection),
        (df2, mapped_connection)
    ), None


def merge_mptcp_dataframes_known_streams(
    con1: Tuple[pd.DataFrame, MpTcpConnection],
    con2: Tuple[pd.DataFrame, MpTcpConnection]
) -> pd.DataFrame:
    """
    Useful for reinjections etc...

    See
        merge_mptcp_dataframes

    Returns:
        Per-subflow dataframes

    """
    df1, main_connection = con1
    df2, mapped_connection = con2

    log.info("Merging mptcp %s with %s" % (main_connection, mapped_connection,))

    df1 = main_connection.fill_dest(df1)
    log.log(mp.TRACE, df1["mptcpdest"].head())
    mapping = map_mptcp_connection_from_known_streams(main_connection, mapped_connection)

    # todo should be inplace
    df_total = pd.DataFrame()
    for sf, mapped_sf in mapping.subflow_mappings:

        df_temp = merge_tcp_dataframes_known_streams(
            (df1, sf),
            (df2, mapped_sf.mapped)
        )

        df_total = pd.concat([df_temp, df_total])

    # we do it a posteriori so that we can still debug a dataframe with full info
    # print(df_total.columns)
    # cols2drop = [ 'tcpflags']
    # cols2drop = _receiver(cols2drop)
    # df_total.drop(labels=cols2drop)

    return df_total


def map_tcp_packet(df, packet, explain=False) -> List[Tuple[Any, float]]:
    # instead should be index ?
    """
    Packets may disappear, get retransmitted

    Args:
        packet:

    Returns:
        a list of tuples (pktid, score)
    """

    def _get_pktid(row) -> int:
        return row.packetid
    # used to be row.Index when df.set_index("packetid") was in use

    def _cmp_packets(p1, p2) -> float:
        """
        returns a score
        """
        # log.debug("comparing packets %s and %s" % (p1, p2))
        score = 0
        # crude approach, packet with most common fields is declared the best
        # log.debug("comparison based on columns %s " % df.columns)
        for field in df.columns:
            try:
                if explain:
                    log.debug("comparing pktids %d with %d for field %s" %
                              (_get_pktid(packet), _get_pktid(row), field))
                f1 = getattr(p1, field)
                f2 = getattr(p2, field)
                score += scoring_rules[field](f1, f2)
                # log.debug("new score after column [%s] = %f" % (field, score))
                if math.isinf(score):
                    if explain:
                        log.debug("Score set to infinity for field %s" % field)
                    break
            except Exception as e:
                pass
                # log.debug("Exception %s" % str(e))

        # when several packets have same dsn/ack number, we add the difference between
        # absolute times so that the first gets a better score to referee between those
        # score -= abs(p2.abstime - p1.abstime)
        return score

    scores = []  # type: List[Any]

    for row in df.itertuples():

        score = _cmp_packets(packet, row)

        # we don't append inf results for performance reasons
        if not math.isinf(score):
            log.debug("packet %d mapped to %d with a score of %d" %
                      (_get_pktid(packet), _get_pktid(row), score))
            scores.append((_get_pktid(row), score))
        # else:
        #     log.debug("Found no match for pktid %d, skipping.." % _get_pktid(packet))

    # sort by score
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores


def print_weird_owds(df):
    """
    Checks for negative owds
    """
    negative_owds = df[df.owd < 0]
    print("Listing")
    temp = ("row sender pktid={packetid" + SENDER_SUFFIX + "}/abstime={abstime" + SENDER_SUFFIX + "}"
            " pktid={packetid_receiver}/abstime={abstime_receiver} owd={owd}")

    for row in negative_owds.itertuples():
        print(temp.format(**row._asdict()))


def map_tcp_packets(
    host1_df, host2_df,
    explain=[],
    mode="hash"
    # con1: TcpConnection, con2: TcpConnection
) -> pd.DataFrame:
    '''
    Dataframe with format
    '''
    if mode == "hash":
        res = map_tcp_packets_via_hash(host1_df, host2_df, explain)
    else:
        res = map_tcp_packets_score_based(host1_df, host2_df, explain)

    log.info("Merged dataframe of size {} generated from {} and {} sources.".format(
        len(res), len(host1_df), len(host2_df)
    ))
    log.info("{} unmapped packets. ".format(
        len(res[res.merge_status == "left_only"]) + len(res[res.merge_status == "right_only"])
    ))

    def _show_unmapped_pkts():
        print(res[res.merge_status == "left_only"])
        print(res[res.merge_status == "right_only"])

    _show_unmapped_pkts()

    return res

def deal_with_duplicated_hash(df):
    '''
    We tell pandas to asser for duplicated packets, thus we need to deal
    with them ourselves
    '''
    log.info("looking for duplicate hashes/packets)")
    dups = df["hash"].duplicated()
    nb_dups = len(df.loc[dups, :])
    # TODO should count the number of duplicated packets
    log.debug("Got %d duplicated hash" % nb_dups)
    # if we had duplicated print them
    if nb_dups > 0:

        log.debug(df.loc[dups, :])
        log.warn("Dropping duplicates in ")
        df.drop_duplicates(
            subset=["hash"],
            inplace=True,
            # TODO maybe we should make it interactive ? or configurable
            keep="first",
        )
    return df


def map_tcp_packets_via_hash(
    # TODO rename, these are not host1/host2 anymore
    host1_df, host2_df,
    *kargs, **kwargs
):
    """
    Merge on hash of different fields
    Resulting dataframe has H1_SUFFIX / H2_SUFFIX
    """
    log.info("Merging dataframes via hash")
    debug_cols = ["packetid", "hash", "abstime"]
    # debug_dataframe(total, "concatenated df",
    #     usecols=_first(["abstime", "tcpdest"]) + _second(["abstime", "tcpdest"]))

    debug_dataframe(host1_df, "host1_df", )
    debug_dataframe(host2_df, "host2 df")

    # todo we could now use merge_asof
    # TODO here we should be able to drop some columns in double
    try:
        # first check hashes are identical
        # check hashes are different

        host1_df = deal_with_duplicated_hash(host1_df)
        host2_df = deal_with_duplicated_hash(host2_df)

        res = pd.merge(
            host1_df, host2_df,
            on="hash",
            suffixes=(HOST1_SUFFIX, HOST2_SUFFIX),  # columns suffixes
            how="outer",  # we want to keep packets from both
            # we want to know how many packets were not mapped correctly, adds the merge column
            # can take values "left_only"/ "right_only" or both
            indicator="merge_status",
            # run additionnal checks against duplicate hashes
            validate="one_to_one",  # can slow process
        )

    except pd.errors.MergeError as e:

        # TODO we don't want to print here
        print("An error happened during the merge of the 2 pcaps")
        print(e)
        raise e

    # TCP_DEBUG_FIELDS
    TCP_DEBUG_FIELDS = ['packetid', "abstime"]
    debug_cols = _first(TCP_DEBUG_FIELDS) + _second(TCP_DEBUG_FIELDS)
    debug_dataframe(res, "Result of merging by hash", usecols=debug_cols)
    return res


def map_tcp_packets_score_based(
    sender_df, receiver_df,
    explain=[],
    mode="hash"
    # con1: TcpConnection, con2: TcpConnection
) -> pd.DataFrame:
    """
    Stream ids must already mapped
    Args:
        explain = increase verbosity for packet ids in this list

    Todo:
        check N = len(sender_df) - len(receiver_df) to know how many packets should be missing,
        then cut off lowest N.

    algo:
        Computes a score on a per packet basis
        Based

    Returns:
        a copy of sender_df with as Index the packetid + a new column called
        "mapped_index" matching the Index of rawdf2
        'rcv_pktid', 'score'
    """
    # DataFrame.add(other, axis='columns', level=None, fill_value=None)
    # adds a new column that contains only nan
    log.debug("Mapping TCP packets between TODO")

    # returns a new object with new columns rcv_pktid, score initialized to NaN
    df_final = sender_df.assign(rcv_pktid=np.nan, score=np.nan,)

    # Problem is to identify lost packets and retransmitted ones
    # so that they map to the same or none ?
    limit = 5  # limit nb of scores to display

    pkt_column = df_final.columns.get_loc('rcv_pktid')
    score_column = df_final.columns.get_loc('score')

    for row_id, row in enumerate(sender_df.itertuples(index=False)):

        explain_pkt = row.packetid in explain
        scores = map_tcp_packet(receiver_df, row, explain_pkt)
        # takes best score index
        # df_final.loc[row.index , 'mapped_index'] = 2 # scores[0][0]
        # print(type(row.Index), type(row.index))
        if len(scores) >= 1:
            print("first %d packets (pandas.index/score)s=\n%s" % (limit, scores[:limit]))
            if explain_pkt:
                for idx, score in scores:
                    log.debug("Score %s=%s" % (idx, score))
            idx, score = scores[0]

            # You should never modify something you are iterating over.
            # print("row.Index=%r (out of %d) and idx=%r" % (row.Index, len(df_final), idx))
            print("row.id=%r (out of %d) and idx=%r" % (row_id, len(df_final), idx))
            # print("df_final.=%r " % (row.Index, idx))
            # df_final.iat[row.Index, df_final.columns.get_loc('rcv_pktid')] = idx
            df_final.iloc[row_id, pkt_column] = idx
            # df_final.loc[row.Index, 'rcv_pktid'] = idx
            # df_final.at[row.Index, 'rcv_pktid'] = idx
            # iat only accepts an integer while iloc can accept a tuple etc
            # print(df_final.iat[row.Index].score)
            df_final.iloc[row_id, score_column] = score
            # TODO we might want to remove that packets from further search

        # drop the chosen index so that it doesn't get used a second time
        # todo pb la c qu'on utilise les packet id comme des index :/
            print("Score %f assigned to index %s" % (score, idx))
            # df2.drop(df2.index[[idx]], inplace=True)
            # df2.drop(idx, inplace=True)
        else:
            # log.debug("No map found for this packet")
            log.debug("Found no match for pktid %d, skipping.." % row.packetid)

        # print("registered = %s" % ( df_final.loc[row.Index, 'mapped_index'])) # , ' at index: ', row.index )

    # checks that
    # if df_final.rcv_pktid.is_unique is False:
    #     log.warn("There seems to be an error: some packets were mapped several times.")

    # print("head=\n", df_final.head())
    return df_final


def map_tcp_stream(rawdf: pd.DataFrame, main: TcpConnection) -> List[TcpMapping]:
    """
    Returns:
        a sorted list of mappings (tcpconnection, score) with the first one being the most probable
    """

    results = []
    for tcpstream in rawdf["tcpstream"].unique():
        other = TcpConnection.build_from_dataframe(rawdf, tcpstream)
        score = main.score(other)
        if score > float('-inf'):
            mapping = TcpMapping(other, score)
            results.append(mapping)

    # decreasing sort based on the score
    results.sort(key=lambda x: x.score, reverse=True)

    return results

def map_mptcp_connection_from_known_streams(
    main: MpTcpConnection,
    other: MpTcpConnection
) -> MpTcpMapping:
    """
    Attempts to map subflows only if score is high enough
    """
    def _map_subflows(main: MpTcpConnection, mapped: MpTcpConnection):
        """
        """
        mapped_subflows = []
        for sf in main.subflows():

            # generates a list (subflow, score)
            scores = list(map(lambda x: TcpMapping(x, sf.score(x)), mapped.subflows()))
            scores.sort(key=lambda x: x.score, reverse=True)
            log.log(mp.TRACE, "sorted scores when mapping %s:\n %r" % (sf, scores))
            mapped_subflows.append((sf, scores[0]))
        return mapped_subflows

    mptcpscore = main.score(other)
    mapped_subflows = None
    if mptcpscore > float('-inf'):
        # (other, score)
        mapped_subflows = _map_subflows(main, other)

    mapping = MpTcpMapping(mapped=other, score=mptcpscore, subflow_mappings=mapped_subflows)
    log.log(mp.TRACE, "mptcp mapping %s", mapping)
    return mapping


def map_mptcp_connection(
    rawdf2: pd.DataFrame, main: MpTcpConnection
) -> List[MpTcpMapping]:
    """
    warn: Do not trust the results yet WIP !

    Returns:
        List of (connection, score) with the best mapping first

    This function tries to map a mptcp.stream from a dataframe (aka pcap) to mptcp.stream
    in another dataframe. For now it just looks at IP level stuff without considering subflow
    mapping score
    """
    log.warning("mapping between datasets is not considered trustable yet")
    results: List[MpTcpMapping] = []

    for mptcpstream2 in rawdf2[_sender("mptcpstream")].dropna().unique():
        other = MpTcpConnection.build_from_dataframe(rawdf2, mptcpstream2)
        mapping = map_mptcp_connection_from_known_streams(main, other)
        results.append(mapping)

    results.sort(key=lambda x: x.score, reverse=True)

    return results


def already_merged(df) -> bool:
    """
    Check if it's a merged dataframes
    """
    # if "redundant" not in df.columns:
    #     return False

    return not df.columns["redundant"].hasnans

def is_merged(df) -> bool:
    return "merge_status" in df.columns


# TODO pass a verbosity level/some stats
def classify_reinjections(df_all: pd.DataFrame) -> pd.DataFrame:
    """
    look at reinjections on the receiver side, see which one is first
    packets with reinjected_in_receiver are (at least they should) be the first DSN arrived.

    Returns:
        a new dataframe with an added column "redundant" and "time_delta"
    """
    log.info("Classifying reinjections")

    if df_all.merged.already_classified():
        log.debug("Already classified, aborting")
        return df_all

    df_all = df_all.assign(redundant=False, reinj_delta=np.nan)

    df = df_all[df_all.merge_status == "both"]

    # print(df_all[ pd.notnull(df_all[_sender("reinjection_of")])] [
    #     _sender(["reinjection_of", "reinjected_in", "packetid", "reltime"]) +
    #     _receiver(["packetid", "reltime"])
    # ])

    for destination in ConnectionRoles:

        log.debug("Looking at mptcp destination %r", destination)
        sender_df = df[df.mptcpdest == destination]

        # print(sender_df[ sender_df.reinjected_in.notna() ][["packetid", "reinjected_in"]])
        # print("successful reinjections" % len(reinjected_in))

        # select only packets that have been reinjected

        # debug_dataframe(sender_df, "reinjections", usecols=["reinjection_of"])
        reinjected_packets = sender_df.dropna(axis='index', subset=[_sender("reinjection_of")])

        log.debug("%d reinjected packets", len(reinjected_packets))
        # with pd.option_context('display.max_rows', None, 'display.max_columns', 300):
        #     print(reinjected_packets[
        #         _sender(["packetid", "reinjected_in", "reinjection_of"])
        #          + _receiver(["reinjected_in", "reinjection_of"])
        #         ].head())

        for reinjection in reinjected_packets.itertuples():
            # here we look at all the reinjected packets

            # print("full reinjection %r" % (reinjection,))

            # if there are packets in _receiver(reinjected_in), it means the reinjections
            # arrived before other similar segments and thus these segments are useless
            # it should work because
            # useless_reinjections = getattr(reinjection, _receiver("reinjected_in"), [])

            # if it was correctly mapped
            if reinjection.merge_status != "both":
                log.log(mp.TRACE, "reinjection %d could not be mapped, giving up...",
                        reinjection.packetid)
                continue

            # print("%r" % reinjection.reinjection_of)
            initial_packetid = reinjection.reinjection_of[0]
            # print("initial_packetid = %r %s" % (initial_packetid, type(initial_packetid)))

            original_packet = df_all.loc[df_all.packetid == initial_packetid].iloc[0]

            if original_packet.merge_status != "both":
                # TODO count missed classifications ?
                log.log(mp.TRACE, "Original packet %d could not be mapped, giving up...",
                        original_packet.packetid)
                continue

            orig_arrival = getattr(original_packet, _receiver("reltime"))
            reinj_arrival = getattr(reinjection, _receiver("reltime"))
            reinj_pktid = getattr(reinjection, _sender("packetid"))

            reinj_delta = orig_arrival - reinj_arrival
            df_all.loc[reinj_pktid, "reinj_delta"] = reinj_delta

            if reinj_delta < pd.Timedelta(0):
                # print("GOT A failed reinjection")
                df_all.loc[df_all[_sender("packetid")] == reinjection.packetid, "redundant"] = True
                #TODO set reinj_delta for reinjection.packetid
            else:
                # print("GOT a successful reinjection")
                pass

    return df_all
